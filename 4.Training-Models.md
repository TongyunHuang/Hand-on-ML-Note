## 1.Linear Regression
Linear regression model prediction
$$\hat{y}  = \theta_{0} + \theta_{1}x_1 + \theta_{2}x_2 + ... + \theta_{n}x_n$$
### 1.1 The Normal Equation
To find the value of $\theta$ that minimizes the cost function, there is a closed form solution. Which is  called the Normal Equation
$$\hat{\theta} = (X^{T}X)^{-1} X^{T}y$$
### 1.2 Computational Complexity
pass

----
## 2.Gradient Descent
![](./img/4.3.png)

- Learning rate: the size of the steps. If small -> many iteration

- Trainig a model neans searching for a combination of model parameters that minimizes a cost function: it's a search in the model's parameter space

### 2.1 Batch Gradient Descent
To implement gradient Descent, we need to compute **partial derivative** - how much the cost function will change if you change $\theta_j$ just a little bit

- partial derivative of the cost fucntion with regard to parameter $\theta_j$
$$\frac{\partial}{\partial\theta_j}MSE(\theta) = \frac{2}{m}\sum_{i-1}^{m}(\theta^Tx^i-y^i)s_j^i$$

<img src="./img/4.6.png" width="250" >

- CONS: The formula involves calculation over the full training set X -> very slow

### 2.2 Stochastic Gradient Descent
- Pick a random instance in the training set at every step and computes the gradients base only on the single instance
- PROS: faster than batch
- CONS: Less regular than batch. The const function will bounce up and down

### 2.3 Mini-batch Gradient Descent
- Computes the gradients on small random sets of instances called mini-batches

----

## 3.Polynomial Regression
pass

## 4.Learning Curves

## 5.Regularized Linear Models

## 6.Logistic Regression